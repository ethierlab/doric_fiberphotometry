{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from ipywidgets import GridBox, Layout\n",
    "\n",
    "import fileselector as fs\n",
    "from importlib import reload\n",
    "reload(fs)\n",
    "fs.load_dependencies()\n",
    "# Usage\n",
    "# Replace '/your/start/directory/' with your actual start directory\n",
    "Path='/home/coder/project/doric_fiberphotometry/Data/knob'\n",
    "\n",
    "file_selector = fs.FileSelector(Path,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = file_selector.get_dataframes()\n",
    "globals().update(dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/coder/project/doric_fiberphotometry/Data/Knob/258_NAc_knob_Apr25_0000_153852396_incomplete/Data/normalized_signal_df_Fail_20240825_183520.csv')\n",
    "# Removing the '.csv' extension from the file name for the variable name\n",
    "var_name = 'normalized_signal_df_Fail'  # Adjust as per your naming convention\n",
    "\n",
    "# Use exec to create a global variable in the Jupyter notebook's namespace\n",
    "globals()[var_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all Signals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileselector as fs\n",
    "from importlib import reload\n",
    "reload(fs)\n",
    "fs.load_dependencies()\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import dataexplorer as de\n",
    "from importlib import reload\n",
    "try:\n",
    "    import ipympl\n",
    "    print(\"ipympl is installed. You can proceed with %matplotlib ipympl.\")\n",
    "    %matplotlib widget\n",
    "except ImportError:\n",
    "    print(\"ipympl is not installed. Please run !pip install ipympl.\")\n",
    "    %pip install ipympl\n",
    "    %matplotlib widget\n",
    "\n",
    "\n",
    "reload(de)\n",
    "\n",
    "import dataexplorer as de\n",
    "from importlib import reload\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "reload(de)\n",
    "from importlib import reload\n",
    "import photometry_functions as pf\n",
    "reload(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Plot All signals\n",
    "y_min = -2\n",
    "y_max = 4\n",
    "\n",
    "import photometry_functions as pf\n",
    "from importlib import reload\n",
    "reload(pf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Init:\n",
    "fig = plt\n",
    "pf.plot_cut_signals(centralized_signals_df_Init,y_min,y_max)\n",
    "# plot_cut_signals(filtered_signal_df)\n",
    "filename = file_selector.file_path.split('.')[0]+'/Figs/'+'/all_Inits.png'\n",
    "\n",
    "directory = os.path.dirname(filename)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"Figure saved as {filename}\")\n",
    "\n",
    "# Success:\n",
    "fig = plt\n",
    "pf.plot_cut_signals(centralized_signals_df_Success,y_min,y_max)\n",
    "# plot_cut_signals(filtered_signal_df)\n",
    "filename = file_selector.file_path.split('.')[0]+'/Figs/'+'/all_Success.png'\n",
    "\n",
    "directory = os.path.dirname(filename)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"Figure saved as {filename}\")\n",
    "\n",
    "# Fail:\n",
    "fig = plt\n",
    "pf.plot_cut_signals(centralized_signals_df_Fail,y_min,y_max)\n",
    "# plot_cut_signals(filtered_signal_df)\n",
    "filename = file_selector.file_path.split('.')[0]+'/Figs/'+'/all_Fails.png'\n",
    "\n",
    "directory = os.path.dirname(filename)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"Figure saved as {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Signals seperatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max=5\n",
    "y_min=-2\n",
    "\n",
    "import photometry_functions as pf\n",
    "from importlib import reload\n",
    "\n",
    "reload(pf)\n",
    "\n",
    "# Init\n",
    "pf.plot_cut_signals_seperated(centralized_signals_df_Init,y_min,y_max)\n",
    "\n",
    "fig = plt\n",
    "filename = file_selector.file_path.split('.')[0]+'/Figs/'+'/Separeted_Init_Events.png'\n",
    "\n",
    "directory = os.path.dirname(filename)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"Figure saved as {filename}\")\n",
    "\n",
    "# Success\n",
    "pf.plot_cut_signals_seperated(centralized_signals_df_Success,y_min,y_max)\n",
    "\n",
    "fig = plt\n",
    "filename = file_selector.file_path.split('.')[0]+'/Figs/'+'/Separeted_Success_Events.png'\n",
    "\n",
    "directory = os.path.dirname(filename)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"Figure saved as {filename}\")\n",
    "\n",
    "# Fail\n",
    "pf.plot_cut_signals_seperated(centralized_signals_df_Fail,y_min,y_max)\n",
    "\n",
    "fig = plt\n",
    "filename = file_selector.file_path.split('.')[0]+'/Figs/'+'/Separeted_Fail_Events.png'\n",
    "\n",
    "directory = os.path.dirname(filename)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"Figure saved as {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and Select Signals for PSTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import photometry_functions as pf\n",
    "from importlib import reload\n",
    "import os\n",
    "\n",
    "reload(pf)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "def handle_filtered_data(df,event_type):\n",
    "    # Handle the filtered DataFrame here, e.g., save it to a global variable or process it further\n",
    "    if event_type == 'Init':    \n",
    "        global filtered_signal_df_Init\n",
    "        filtered_signal_df_Init = df\n",
    "        print(filtered_signal_df_Init.head())\n",
    "    \n",
    "    if event_type == 'Fail':    \n",
    "        global filtered_signal_df_Fail\n",
    "        filtered_signal_df_Fail = df\n",
    "        print(filtered_signal_df_Fail.head())\n",
    "    \n",
    "    if event_type == 'Success':    \n",
    "        global filtered_signal_df_Success\n",
    "        filtered_signal_df_Success = df\n",
    "        print(filtered_signal_df_Success.head())\n",
    "    \n",
    "    \n",
    "    fig = plt\n",
    "    filename = file_selector.file_path.split('.')[0]+'/Figs/'+'/Selected_Stim_Events_'+event_type+'.png'\n",
    "\n",
    "    directory = os.path.dirname(filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    fig.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Figure saved as {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "# Init:\n",
    "explorer = pf.SignalExplorer(centralized_signals_df_Init,'Init', save_callback=handle_filtered_data)\n",
    "# This will display the widgets and allow interaction\n",
    "\n",
    "# Success:\n",
    "explorer = pf.SignalExplorer(centralized_signals_df_Success,'Success', save_callback=handle_filtered_data)\n",
    "# This will display the widgets and allow interaction\n",
    "\n",
    "# Fail:\n",
    "explorer = pf.SignalExplorer(centralized_signals_df_Fail,'Fail', save_callback=handle_filtered_data)\n",
    "# This will display the widgets and allow interaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import photometry_functions as pf\n",
    "from importlib import reload\n",
    "\n",
    "reload(pf)\n",
    "\n",
    "\n",
    "# Init:\n",
    "# Assuming 'Data' is the column in filtered_signal_df that you want to normalize\n",
    "# normalized_signal_df_Init = pf.normalize_signal(filtered_signal_df_Init, column='Data')\n",
    "normalized_signal_df_Init = filtered_signal_df_Init\n",
    "\n",
    "# Success:\n",
    "# Assuming 'Data' is the column in filtered_signal_df that you want to normalize\n",
    "# normalized_signal_df_Success = pf.normalize_signal(filtered_signal_df_Success, column='Data')\n",
    "normalized_signal_df_Success = filtered_signal_df_Success\n",
    "\n",
    "# Fail:\n",
    "# Assuming 'Data' is the column in filtered_signal_df that you want to normalize\n",
    "# normalized_signal_df_Fail = pf.normalize_signal(filtered_signal_df_Fail, column='Data')\n",
    "normalized_signal_df_Fail = filtered_signal_df_Fail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reloading saved data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataframe_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Loads a DataFrame from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file to be loaded.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    # Loading the DataFrame from the specified CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'your_file.csv' with the path to the CSV file you want to load\n",
    "\n",
    "csv_file_path = 'your_file.csv'\n",
    "df = load_dataframe_from_csv(csv_file_path)\n",
    "# Optionally, you can display the first few rows of the loaded DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Resaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "\n",
    "class TimeDomainAnalyzer:\n",
    "    def __init__(self, signal_df):\n",
    "        self.signal_df = signal_df\n",
    "\n",
    "    def analyze_event(self, event_df):\n",
    "        features = {\n",
    "            'Peak Amplitude': event_df['Data'].max(),\n",
    "            'Minimum Amplitude': event_df['Data'].min(),\n",
    "            'Mean Signal': event_df['Data'].mean(),\n",
    "            'Median Signal': event_df['Data'].median(),\n",
    "            'Standard Deviation': event_df['Data'].std(),\n",
    "            'Variance': event_df['Data'].var(),\n",
    "            'Skewness': skew(event_df['Data']),\n",
    "            'Kurtosis': kurtosis(event_df['Data']),\n",
    "            'Rise Time': self.calculate_rise_time(event_df),\n",
    "            'Decay Time': self.calculate_decay_time(event_df),\n",
    "            'FWHM': self.calculate_fwhm(event_df),\n",
    "            'AUC': np.trapz(event_df['Data'], event_df['Time']),                #calculate abs value    \n",
    "            'Baseline Value': event_df['Data'].iloc[0],                         #baseline should be calculated based on trend line on the whole signal\n",
    "            'Baseline Drift': event_df['Data'].iloc[-1] - event_df['Data'].iloc[0],\n",
    "            'SNR': self.calculate_snr(event_df),\n",
    "            'Event Duration': event_df['Time'].iloc[-1] - event_df['Time'].iloc[0],\n",
    "            'Zero-Crossing Rate': self.calculate_zero_crossings(event_df)\n",
    "        }\n",
    "        return pd.DataFrame([features])\n",
    "\n",
    "    def calculate_rise_time(self, event_df):\n",
    "        # Custom method to calculate rise time\n",
    "        # Implement logic specific to your signal\n",
    "        return np.nan\n",
    "\n",
    "    def calculate_decay_time(self, event_df):\n",
    "        # Custom method to calculate decay time\n",
    "        # Implement logic specific to your signal\n",
    "        return np.nan\n",
    "\n",
    "    def calculate_fwhm(self, event_df):\n",
    "        # Custom method to calculate Full Width at Half Maximum\n",
    "        # Implement logic specific to your signal\n",
    "        return np.nan\n",
    "\n",
    "    def calculate_snr(self, event_df):\n",
    "        signal_power = np.mean(event_df['Data']**2)\n",
    "        noise_power = np.var(event_df['Data'])\n",
    "        return 10 * np.log10(signal_power / noise_power)\n",
    "\n",
    "    def calculate_zero_crossings(self, event_df):\n",
    "        zero_crossings = np.where(np.diff(np.sign(event_df['Data'])))[0]\n",
    "        return len(zero_crossings)\n",
    "\n",
    "    def analyze(self):\n",
    "        results = {}\n",
    "        for event_id, event_df in self.signal_df.groupby(level=0):\n",
    "            results[event_id] = self.analyze_event(event_df)\n",
    "        return pd.concat(results.values(), keys=results.keys())\n",
    "\n",
    "# Usage Example:\n",
    "start=-0.8\n",
    "end=0\n",
    "filtered_df_Success = normalized_signal_df_Success[(normalized_signal_df_Success['Time'] >= start) & (normalized_signal_df_Success['Time'] <= end)]\n",
    "time_analyzer = TimeDomainAnalyzer(filtered_df_Success)\n",
    "time_results_Success = time_analyzer.analyze()\n",
    "filtered_df_Fail = normalized_signal_df_Fail[(normalized_signal_df_Fail['Time'] >= start) & (normalized_signal_df_Fail['Time'] <= end)]\n",
    "time_analyzer = TimeDomainAnalyzer(filtered_df_Fail)\n",
    "time_results_Fail = time_analyzer.analyze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_results_Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_results_Fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the data is reset as per your initial steps\n",
    "time_results_Fail = time_results_Fail.reset_index(drop=True)\n",
    "time_results_Success = time_results_Success.reset_index(drop=True)\n",
    "\n",
    "# Create a directory for saving the figures if it doesn't exist\n",
    "output_dir = 'Figs'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Plotting each feature with all data points as dots and min, max, mean\n",
    "for feature in time_results_Fail.columns:\n",
    "    # Create the figure and axis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Define the positions for the Fail and Success regions\n",
    "    positions_fail = np.full(time_results_Fail[feature].shape, 1)\n",
    "    positions_success = np.full(time_results_Success[feature].shape, 2)\n",
    "    \n",
    "    # Plot all data points for Fail and Success datasets\n",
    "    plt.scatter(positions_fail, time_results_Fail[feature], color='b', label='Fail Data', alpha=0.6)\n",
    "    plt.scatter(positions_success, time_results_Success[feature], color='r', label='Success Data', alpha=0.6)\n",
    "    \n",
    "    # Calculate and plot the min, max, and mean for Fail\n",
    "    fail_min = time_results_Fail[feature].min()\n",
    "    fail_max = time_results_Fail[feature].max()\n",
    "    fail_mean = time_results_Fail[feature].mean()\n",
    "\n",
    "    plt.plot([0.8, 1.2], [fail_min, fail_min], color='b', linestyle='--', label=f'Fail Min: {fail_min:.2f}')\n",
    "    plt.plot([0.8, 1.2], [fail_max, fail_max], color='b', linestyle='-', label=f'Fail Max: {fail_max:.2f}')\n",
    "    plt.plot([0.8, 1.2], [fail_mean, fail_mean], color='b', linestyle=':', label=f'Fail Mean: {fail_mean:.2f}')\n",
    "    \n",
    "    # Calculate and plot the min, max, and mean for Success\n",
    "    success_min = time_results_Success[feature].min()\n",
    "    success_max = time_results_Success[feature].max()\n",
    "    success_mean = time_results_Success[feature].mean()\n",
    "\n",
    "    plt.plot([1.8, 2.2], [success_min, success_min], color='r', linestyle='--', label=f'Success Min: {success_min:.2f}')\n",
    "    plt.plot([1.8, 2.2], [success_max, success_max], color='r', linestyle='-', label=f'Success Max: {success_max:.2f}')\n",
    "    plt.plot([1.8, 2.2], [success_mean, success_mean], color='r', linestyle=':', label=f'Success Mean: {success_mean:.2f}')\n",
    "    \n",
    "    # Add labels, title, and legend\n",
    "    plt.title(f'{feature} in Time Results')\n",
    "    plt.xticks([1, 2], ['Fail', 'Success'])\n",
    "    plt.ylabel(feature)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    # Save the figure\n",
    "    filename = os.path.join(output_dir, f'time-analysis-dot-{feature}.png')\n",
    "    plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Figure saved as {filename}\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index if it's multi-index or has an additional level\n",
    "time_results_Fail = time_results_Fail.reset_index(drop=True)\n",
    "time_results_Success = time_results_Success.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(9, 2, figsize=(12, 18))\n",
    "# print(f\"Figure saved as {filename}\")\n",
    "for feature in time_results_Fail.columns:\n",
    "    # Now, plot the Peak Amplitude again\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time_results_Fail.index, time_results_Fail[feature], marker='o', linestyle='-', color='b')\n",
    "    plt.plot(time_results_Success.index, time_results_Success[feature], marker='o', linestyle='-', color='r')\n",
    "    plt.title(feature+' in Time Results Fail')\n",
    "    plt.xlabel('Event')\n",
    "    plt.ylabel(feature)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    filename = file_selector.file_path.split('.')[0]+'/Figs/'+'time-analysis-'+feature+'.png'\n",
    "    directory = os.path.dirname(filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Figure saved as {filename}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Ensure 'Peak Amplitude' is fully numeric and convert anything else to NaN\n",
    "time_results_Fail['Peak Amplitude'] = pd.to_numeric(time_results_Fail['Peak Amplitude'], errors='coerce')\n",
    "\n",
    "# Drop any rows where 'Peak Amplitude' is NaN\n",
    "time_results_Fail = time_results_Fail.dropna(subset=['Peak Amplitude'])\n",
    "\n",
    "# Double-check the data type of 'Peak Amplitude'\n",
    "print(time_results_Fail['Peak Amplitude'].dtype)\n",
    "\n",
    "# Plot the Peak Amplitude again\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_results_Fail.index, time_results_Fail['Peak Amplitude'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Peak Amplitude in Time Results Fail')\n",
    "plt.xlabel('Event')\n",
    "plt.ylabel('Peak Amplitude')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving all the Dataframes as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def save_dataframes_to_csv():\n",
    "    # Fetch the current datetime to append to file names to avoid overwriting\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = file_selector.file_path.split('.')[0]+'/Data/'+'test.csv\"'\n",
    "    directory = os.path.dirname(filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    # Loop through all global variables\n",
    "    for var_name, value in list(globals().items()):\n",
    "        # Check if the value is an instance of pd.DataFrame\n",
    "        if isinstance(value, pd.DataFrame):\n",
    "            # Construct filename with a timestamp to prevent overwriting\n",
    "            file = f\"{var_name}_{now}.csv\"\n",
    "            filename = file_selector.file_path.split('.')[0]+'/Data/'+file\n",
    "\n",
    "            # Save the DataFrame to a CSV file\n",
    "            value.to_csv(filename, index=False)\n",
    "            print(f\"Saved {filename}\")\n",
    "\n",
    "\n",
    "# Call the function to save all DataFrames\n",
    "save_dataframes_to_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reloading saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataframe_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Loads a DataFrame from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file to be loaded.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    # Loading the DataFrame from the specified CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'your_file.csv' with the path to the CSV file you want to load\n",
    "\n",
    "csv_file_path = 'your_file.csv'\n",
    "df = load_dataframe_from_csv(csv_file_path)\n",
    "# Optionally, you can display the first few rows of the loaded DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized_signals_df_Init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "# Assuming centralized_signals_df_Init is your DataFrame\n",
    "\n",
    "# If 'Event' is part of a MultiIndex, reset the index\n",
    "if isinstance(centralized_signals_df_Init.index, pd.MultiIndex):\n",
    "    centralized_signals_df_Init.reset_index(inplace=True)\n",
    "\n",
    "# Filter the DataFrame for the base data at Time 0\n",
    "# base_df = centralized_signals_df_Init[centralized_signals_df_Init['Time'] == 0]\n",
    "# Filter the dataframe for the time range first\n",
    "base_df = centralized_signals_df_Init[(centralized_signals_df_Init['Time'] >= -4) & (centralized_signals_df_Init['Time'] <= -2)]\n",
    "\n",
    "\n",
    "\n",
    "base_df = base_df.groupby('Event')['Data'].mean().reset_index()\n",
    "base_df.rename(columns={'Data': 'F_Init'}, inplace=True)\n",
    "\n",
    "# Filter the DataFrame for Time values between 0 and 2\n",
    "filtered_df = centralized_signals_df_Init[(centralized_signals_df_Init['Time'] >= 0) & (centralized_signals_df_Init['Time'] <= 2)]\n",
    "\n",
    "# Group by 'Event' and find the maximum 'Data' value in each group\n",
    "max_df = filtered_df.groupby('Event')['Data'].max().reset_index()\n",
    "max_df.rename(columns={'Data': 'F_Max'}, inplace=True)\n",
    "\n",
    "# Merge the base data and MaxdFF\n",
    "result_df = pd.merge(base_df, max_df, on='Event', how='left')\n",
    "\n",
    "# Calculate the difference between 'Max Data' and 'Base'\n",
    "result_df['dFF'] = result_df['F_Max'] - result_df['F_Init']\n",
    "\n",
    "# Calculate Init_Speed and Av_Speed\n",
    "# Define time intervals for Init_Speed and Av_Speed calculations\n",
    "init_interval_df = centralized_signals_df_Init[(centralized_signals_df_Init['Time'] >= 0) & (centralized_signals_df_Init['Time'] <= 0.1)]\n",
    "av_interval_df = filtered_df  # already filtered from 0 to 2\n",
    "\n",
    "# Group by 'Event' and calculate min and max 'Data' for Init_Speed\n",
    "init_speed_calc = init_interval_df.groupby('Event')['Data'].agg(['min', 'max'])\n",
    "init_speed_calc['F_Init_Speed'] = (init_speed_calc['max'] - init_speed_calc['min']) / 0.1\n",
    "\n",
    "# Group by 'Event' and calculate min and max 'Data' for Av_Speed\n",
    "av_speed_calc = av_interval_df.groupby('Event')['Data'].agg(['min', 'max'])\n",
    "av_speed_calc['F_Av_Speed'] = (av_speed_calc['max'] - av_speed_calc['min']) / 2\n",
    "\n",
    "# Merge these calculations into the result_df\n",
    "result_df = result_df.merge(init_speed_calc[['F_Init_Speed']], on='Event', how='left')\n",
    "result_df = result_df.merge(av_speed_calc[['F_Av_Speed']], on='Event', how='left')\n",
    "\n",
    "# Save to CSV\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "file = f\"result_data_{now}.csv\"\n",
    "result_df.to_csv(file, index=False)\n",
    "result_df.to_csv('result_data.csv', index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"The result has been saved to '{file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simulating the creation of your initial DataFrame\n",
    "data = {\n",
    "    \"Event\": [1, 1, 1, 1, 1],\n",
    "    \"Row\": [617, 618, 619, 620, 621],\n",
    "    \"Time\": [-4.00, -3.99, -3.98, -3.97, -3.96],\n",
    "    \"Data\": [0.330067, 0.341075, 0.356469, 0.363225, 0.341833]\n",
    "}\n",
    "centralized_signals_df_Init = pd.DataFrame(data)\n",
    "\n",
    "# Define the time intervals\n",
    "init_interval = (0, 0.1)\n",
    "av_interval = (0, 2)\n",
    "\n",
    "# Filter the DataFrame for the initial interval and calculate Init_Speed\n",
    "init_data = centralized_signals_df_Init[(centralized_signals_df_Init['Time'] >= init_interval[0]) & (centralized_signals_df_Init['Time'] <= init_interval[1])]\n",
    "if not init_data.empty:\n",
    "    init_speed = (init_data['Data'].max() - init_data['Data'].min()) / (init_data['Time'].max() - init_data['Time'].min())\n",
    "else:\n",
    "    init_speed = 0\n",
    "\n",
    "# Filter the DataFrame for the average interval and calculate Av_Speed\n",
    "av_data = centralized_signals_df_Init[(centralized_signals_df_Init['Time'] >= av_interval[0]) & (centralized_signals_df_Init['Time'] <= av_interval[1])]\n",
    "if not av_data.empty:\n",
    "    av_speed = (av_data['Data'].max() - av_data['Data'].min()) / (av_data['Time'].max() - av_data['Time'].min())\n",
    "else:\n",
    "    av_speed = 0\n",
    "\n",
    "# Assuming result_df exists and we can add columns to it\n",
    "result_df['Init_Speed'] = init_speed\n",
    "result_df['Av_Speed'] = av_speed\n",
    "\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming centralized_signals_df_Init is your DataFrame\n",
    "\n",
    "# Reset index if 'Event' is part of a MultiIndex\n",
    "if isinstance(centralized_signals_df_Init.index, pd.MultiIndex):\n",
    "    centralized_signals_df_Init.reset_index(inplace=True)\n",
    "\n",
    "# Define function to calculate speed\n",
    "def calculate_speed(df, start_time, end_time):\n",
    "    # Filter data within the specified time range\n",
    "    time_filtered = df[(df['Time'] >= start_time) & (df['Time'] <= end_time)]\n",
    "    if time_filtered.empty:\n",
    "        return float('nan')  # Return NaN if no data in the specified range\n",
    "    else:\n",
    "        # Calculate max and min data values\n",
    "        max_value = time_filtered['Data'].max()\n",
    "        min_value = time_filtered['Data'].min()\n",
    "        # Calculate dt\n",
    "        dt = end_time - start_time\n",
    "        # Calculate speed\n",
    "        return (max_value - min_value) / dt\n",
    "\n",
    "result_df['Init_Speed'] = calculate_speed(centralized_signals_df_Init, 0, 0.1)\n",
    "result_df['Av_Speed'] = calculate_speed(centralized_signals_df_Init, 0, 2)\n",
    "\n",
    "# Save to CSV\n",
    "result_df.to_csv('result_data_with_speeds.csv', index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"The result, including 'Init_Speed' and 'Av_Speed', has been saved to 'result_data_with_speeds.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
